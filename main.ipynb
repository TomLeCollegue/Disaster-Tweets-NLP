{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster Tweets\n",
    "#### `Hugo Hersemeule, Tom Kubasik`\n",
    "#### Enoncé Kaggle: https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "train_data = pandas.read_csv(\"./data/train.csv\")\n",
    "test_data = pandas.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'une des principale difficulté dans le NLP est qu'un algorithme de machine learning ne peut pas traiter des mots, il ne peut traiter que des données numériques. \n",
    "\n",
    "On peut utiliser CountVectorizer() pour transformer nos tweets en données numérique.\n",
    "Il convertit une serie de text en une matrix de comptage de mots.\n",
    "\n",
    "On utilise 'fit_transform' sur les data d'entrainement, et 'transform' sur les données de test afin de pouvoir tester les données de test avec la même base de mot que les données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_data[\"text\"])\n",
    "test_vectors = count_vectorizer.transform(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ces données on fait une régression Ridge.\n",
    "On peut voir que l'on optient un score en moyenne entre 50 et 60% de bonne reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6025641  0.50126582 0.56985004 0.50781969 0.67275495]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.RidgeClassifier()\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train_data[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(scores)\n",
    "\n",
    "clf.fit(train_vectors, train_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des ammélioration de countVectorizer est d'utiliser la technique de Stemming, cela consiste a enlever les suffix des mots, ainsi \"troubling\", \"troubled\", et \"trouble\" diviennent \"troubl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tomkubasik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troubl\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=stemSentence(train_data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "our deed are the reason of thi # earthquak may allah forgiv us all \n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"text\"][0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemed_train_data = map(stemSentence, train_data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique le stemming a toutes les phrases et on refait le countVertorizer(), on peut voir une petit augmentation de la fiabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59170507 0.52796053 0.58447489 0.57529611 0.68711656]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_stem = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors_stem = count_vectorizer_stem.fit_transform(stemed_train_data)\n",
    "\n",
    "clf_stem = linear_model.RidgeClassifier()\n",
    "scores_stem = model_selection.cross_val_score(clf_stem, train_vectors_stem, train_data[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(scores_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre chose possible est d'utiliser la technique de Lematization, cela consiste a regrouper les mots par champs lexicals, ici, on le fait sur les verbe, ainsi \"Were\", \"was\", \"is\" devient \"Be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tomkubasik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tomkubasik/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Our Deeds be the Reason of this earthquake May ALLAH Forgive us all \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "punctuations=\"?:!.,;#\"\n",
    "\n",
    "def lemmaSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    for word in token_words:\n",
    "        if word in punctuations:\n",
    "            token_words.remove(word)\n",
    "    \n",
    "    lem_sentence=[]\n",
    "    for word in token_words:\n",
    "        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
    "        lem_sentence.append(\" \")\n",
    "    return \"\".join(lem_sentence)\n",
    "\n",
    "lem=lemmaSentence(train_data[\"text\"][0])\n",
    "print(train_data[\"text\"][0])\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59170507 0.52796053 0.58447489 0.57529611 0.68711656]\n"
     ]
    }
   ],
   "source": [
    "lem_train_data = map(lemmaSentence, train_data[\"text\"])\n",
    "\n",
    "count_vectorizer_lem = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "train_vectors_lem = count_vectorizer_lem.fit_transform(lem_train_data)\n",
    "\n",
    "clf_lem = linear_model.RidgeClassifier()\n",
    "scores_lem = model_selection.cross_val_score(clf_lem, train_vectors_lem, train_data[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(scores_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les données on nous donne une localisation sur certain de tweet, une chose interessante serait de regarder si le fait de mettre une localisation ou non est interessant dans la recherche de \"Disaster tweets\".\n",
    "\n",
    "Si on a une localisation, on met un YES, si on n'a pas de localisation, on met un NO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       NO\n",
      "1       NO\n",
      "2       NO\n",
      "3       NO\n",
      "4       NO\n",
      "        ..\n",
      "7608    NO\n",
      "7609    NO\n",
      "7610    NO\n",
      "7611    NO\n",
      "7612    NO\n",
      "Name: location, Length: 7613, dtype: object\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "def replaceTextWith1(sentence):\n",
    "    if sentence == 0:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"YES\"\n",
    "\n",
    "train_data[\"location\"] = train_data[\"location\"].fillna(0)\n",
    "train_data[\"location\"] = list(map(replaceTextWith1, train_data[\"location\"]))\n",
    "\n",
    "# Verif printing\n",
    "print(train_data[\"location\"])\n",
    "print(train_data[\"location\"][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, on tente d'entrainer le modele sur deux parametre, la presence ou non de localisation et le texte lematizé. Pour cela on combinent les deux features et on fait une regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Our Deeds be the Reason of this earthquake May...\n",
      "1                  Forest fire near La Ronge Sask Canada \n",
      "2       All residents ask to 'shelter in place ' be be...\n",
      "3       13,000 people receive wildfires evacuation ord...\n",
      "4       Just get send this photo from Ruby Alaska as s...\n",
      "                              ...                        \n",
      "7608    Two giant crane hold a bridge collapse into ne...\n",
      "7609    @ aria_ahrary @ TheTawniest The out of control...\n",
      "7610    M1.94 [ 01:04 UTC ] 5km S of Volcano Hawaii ht...\n",
      "7611    Police investigate after an e-bike collide wit...\n",
      "7612    The Latest More Homes Razed by Northern Califo...\n",
      "Name: text, Length: 7613, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data[\"text\"] = list(map(lemmaSentence, train_data[\"text\"]))\n",
    "print(train_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    " \n",
    "\n",
    "# traitement du text\n",
    "vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vector_final = vectorizer.fit_transform(train_data[\"text\"])\n",
    "\n",
    "vectorizer_location = feature_extraction.text.CountVectorizer()\n",
    "train_vector_location = vectorizer_location.fit_transform(train_data[\"location\"])\n",
    "\n",
    "combined_features= hstack([train_vector_final, train_vector_location], 'csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59645853 0.53078203 0.5797546  0.56091148 0.68735806]\n"
     ]
    }
   ],
   "source": [
    "clf_final = linear_model.RidgeClassifier()\n",
    "scores_final = model_selection.cross_val_score(clf_final, combined_features, train_data[\"target\"], cv=5, scoring=\"f1\")\n",
    "print(scores_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que qu'il y a peut d'ammélioration... on peut en deduire que la presence de localisation n'influe pas ou peut sur la nature du Tweet, peut etre qu'il faudrait prendre les localisations en elle même plutot qu'un boolean \"Localisation indiquée ou non\". Pour continuer on pourrait regarder la presence ou non de majuscule dans les tweets, les #Hashtag qui sont propres a la plateforme, Il reste une donnée non exploitée qui est la collomne \"keyword\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
